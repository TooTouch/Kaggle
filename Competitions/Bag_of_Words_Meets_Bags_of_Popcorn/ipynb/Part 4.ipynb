{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part1~3 정리 및 모델 개선\n",
    "\n",
    "앞선 Part1~3 을 통해서 3가지 전처리 방법과 Random Forest를 통해 성능을 테스트하였다. \n",
    "1. Part1: bag of words (CounterVectorizer) ```review 개수 X max_features```\n",
    "2. Part2: Word2Vec을 통해 review별 단어들의 vector를 평균으로하여 사용 ```review 개수 X max_features```\n",
    "3. Part3: K-means clustering을 통한 review별 단어들의 군집을 count하여 feature 생성 ```review 개수 X cluster 개수```\n",
    "\n",
    "Public Leaderboard Score\n",
    "\n",
    "Name | Binary | Proba\n",
    "-------|-------|-------\n",
    "CounterVectorizer | 0.84392 | 0.92104\n",
    "Word2Vec AverageVectors | 0.78028 | 0.85884\n",
    "Word2Vec AverageVectors stemming | 0.81984 | 0.89700\n",
    "Bag Of Centroids | 0.80816 | 0.88930\n",
    "BagOfCentroids_stopwords | 0.81192 | 0.89310\n",
    "\n",
    "\n",
    "\n",
    "이번 notebook에서는 시퀀스 모델링을 통한 성능개선을 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from KaggleWord2VecUtility import KaggleWord2VecUtility\n",
    "kaggle_utils = KaggleWord2VecUtility()\n",
    "## Pipeline 1\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "## Pipeline 3\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# multiprocessing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# word2vec\n",
    "## PIpeline 2 & 3 \n",
    "from gensim.models import word2vec\n",
    "\n",
    "# graphs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# model\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# evaluation\n",
    "from sklearn import metrics\n",
    "\n",
    "# warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (25000, 3)\n",
      "unlabeled_train shape: (50000, 2)\n",
      "test shape:  (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "# quoting = 3 은 \n",
    "train = pd.read_csv('../dataset/labeledTrainData.tsv',delimiter='\\t', quoting=3)\n",
    "unlabeled_train = pd.read_csv('../dataset/unlabeledTrainData.tsv', delimiter='\\t', quoting=3)\n",
    "test = pd.read_csv('../dataset/testData.tsv',delimiter='\\t')\n",
    "print('train shape: ',train.shape)\n",
    "print('unlabeled_train shape:',unlabeled_train.shape)\n",
    "print('test shape: ',test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline1(data, workers, train=False):\n",
    "    x_data = kaggle_utils.getCleanReviews(reviews=data, \n",
    "                                          func=kaggle_utils.review_to_join_words,\n",
    "                                          workers=workers)\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None,\n",
    "                             stop_words = None,   \n",
    "                             max_features = 5000) \n",
    "    x_data = vectorizer.fit_transform(x_data).toarray()\n",
    "    if train:\n",
    "        return x_data, data['sentiment']\n",
    "    else:\n",
    "        return x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline2(data, workers, train=False):\n",
    "    model = word2vec.Word2Vec.load(\"../saved_model/300features_40minwords_10context_add_stemming\")\n",
    "    x_data = kaggle_utils.getCleanReviews(reviews=data, \n",
    "                                          func=kaggle_utils.review_to_wordlist,\n",
    "                                          workers=workers)\n",
    "    x_data = kaggle_utils.getAvgFeatureVecs(x_data, model, 300)\n",
    "    if train:\n",
    "        return x_data, data['sentiment']\n",
    "    else:\n",
    "        return x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline3(train, test, workers, num_words):\n",
    "    model = word2vec.Word2Vec.load(\"../saved_model/300features_40minwords_10context_add_stemming\")\n",
    "    word_vectors = model.wv.syn0\n",
    "    num_clusters = int(word_vectors.shape[0] / num_words)\n",
    "\n",
    "    # Initalize a k-means object and use it to extract centroids\n",
    "    kmeans_clustering = KMeans( n_clusters = num_clusters , n_jobs=workers)\n",
    "    idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "    \n",
    "    # Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "    # a cluster number    \n",
    "    word_centroid_map = dict(zip( model.wv.index2word, idx ))\n",
    "    \n",
    "    # clean_reviews to wordlist\n",
    "    train_clean_reviews = kaggle_utils.getCleanReviews(reviews=train, \n",
    "                                                     func=kaggle_utils.review_to_wordlist,\n",
    "                                                     workers=workers)\n",
    "    test_clean_reviews = kaggle_utils.getCleanReviews(reviews=test, \n",
    "                                                     func=kaggle_utils.review_to_wordlist,\n",
    "                                                     workers=workers)\n",
    "    \n",
    "    # Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "    x_data = np.zeros((train[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "    x_test = np.zeros((test[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "    # Transform the training set reviews into bags of centroids\n",
    "    counter = 0\n",
    "    for review in train_clean_reviews:\n",
    "        x_data[counter] = kaggle_utils.create_bag_of_centroids( review, word_centroid_map )\n",
    "        counter += 1\n",
    "        \n",
    "    counter = 0\n",
    "    for review in test_clean_reviews:\n",
    "        x_test[counter] = kaggle_utils.create_bag_of_centroids( review, word_centroid_map )\n",
    "        counter += 1\n",
    "    \n",
    "    \n",
    "    return x_data, train['sentiment'], x_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data1, y_data = pipeline1(train, workers=12, train=True)\n",
    "x_test1 = pipeline1(test, workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n",
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n"
     ]
    }
   ],
   "source": [
    "x_data2, y_data = pipeline2(train, workers=12, train=True)\n",
    "x_test2 = pipeline2(test, workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data3, y_data, x_test3 = pipeline3(train, test, workers=12, num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data1.shape: (25000, 5000)  / x_test1.shape: (25000, 5000)\n",
      "x_data2.shape: (25000, 300)  / x_test2.shape: (25000, 300)\n",
      "x_data3.shape: (25000, 2397)  / x_test3.shape: (25000, 2397)\n"
     ]
    }
   ],
   "source": [
    "print('x_data1.shape: {}  / x_test1.shape: {}'.format(x_data1.shape, x_test1.shape))\n",
    "print('x_data2.shape: {}  / x_test2.shape: {}'.format(x_data2.shape, x_test2.shape))\n",
    "print('x_data3.shape: {}  / x_test3.shape: {}'.format(x_data3.shape, x_test3.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"seed\":223,\n",
    "    \"k_folds\":5,\n",
    "    \"early_stopping_rounds\":100\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"n_estimators\": 10000,\n",
    "    \"max_depth\": 3,\n",
    "    \"min_child_weight\": 3,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 1.0,\n",
    "    \"colsample_bylevel\": 1.0,\n",
    "    \"alpha\": 0,\n",
    "    \"lambda\": 1,\n",
    "    \"objective\": \"gpu:binary:logistic\",\n",
    "    \"tree_method\": \"gpu_hist\",\n",
    "    \"predictor\": \"gpu_predictor\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_kfold(x_data, y_data, x_test, folds):\n",
    "    xgb_auc_list = list()\n",
    "    xgb_acc_list = list()\n",
    "    xgb_S_prediction = np.zeros(len(x_data))\n",
    "    xgb_prediction = np.zeros(len(x_test))\n",
    "\n",
    "    for fold, (train_idx, valid_idx) in enumerate(folds.split(X=x_data, y=y_data)):\n",
    "        x_train, y_train = x_data[train_idx,:], y_data[train_idx]\n",
    "        x_valid, y_valid = x_data[valid_idx,:], y_data[valid_idx] \n",
    "        xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "        xgb_model.fit(x_train, y_train, \n",
    "                  eval_set=[(x_valid, y_valid)],  \n",
    "                  eval_metric=\"auc\",\n",
    "                  early_stopping_rounds=config['early_stopping_rounds'], \n",
    "                  verbose=False)\n",
    "\n",
    "        prob = xgb_model.predict_proba(x_valid, ntree_limit=xgb_model.best_iteration)[:,1]\n",
    "        test_prob = xgb_model.predict_proba(x_test, ntree_limit=xgb_model.best_iteration)[:,1]\n",
    "\n",
    "        auc = metrics.roc_auc_score(y_true=y_valid, y_score=prob)\n",
    "        pred = [1 if p > 0.5 else 0 for p in prob]\n",
    "        acc = metrics.accuracy_score(y_true=y_valid, y_pred=pred)\n",
    "        xgb_auc_list.append(auc)\n",
    "        xgb_acc_list.append(acc)\n",
    "\n",
    "        xgb_S_prediction[valid_idx] = prob\n",
    "        xgb_prediction += test_prob / folds.n_splits\n",
    "\n",
    "        print(\"{} fold's AUC: {}\".format(fold+1, auc))\n",
    "        print(\"{} fold's ACC: {}\".format(fold+1, acc))\n",
    "\n",
    "    print('='*100)\n",
    "    print('AUC_list')\n",
    "    print(xgb_auc_list)\n",
    "\n",
    "    print('-'*100)\n",
    "    print('Mean AUC: {}'.format(np.mean(xgb_auc_list)))\n",
    "\n",
    "    print('='*100)\n",
    "    print('ACC_list')\n",
    "    print(xgb_acc_list)\n",
    "\n",
    "    print('-'*100)\n",
    "    print('Mean ACC: {}'.format(np.mean(xgb_acc_list)))\n",
    "    \n",
    "    return xgb_prediction, xgb_auc_list, xgb_acc_list, xgb_S_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:45:45] WARNING: C:/dev/libs/xgboost/src/objective/regression_obj.cu:170: gpu:binary:logistic is now deprecated, use binary:logistic instead.\n",
      "1 fold's AUC: 0.9519849217123374\n",
      "1 fold's ACC: 0.8848\n",
      "[17:46:24] WARNING: C:/dev/libs/xgboost/src/objective/regression_obj.cu:170: gpu:binary:logistic is now deprecated, use binary:logistic instead.\n",
      "2 fold's AUC: 0.9487462390371935\n",
      "2 fold's ACC: 0.8776\n",
      "[17:46:55] WARNING: C:/dev/libs/xgboost/src/objective/regression_obj.cu:170: gpu:binary:logistic is now deprecated, use binary:logistic instead.\n",
      "3 fold's AUC: 0.9466345726151382\n",
      "3 fold's ACC: 0.8764\n",
      "[17:47:34] WARNING: C:/dev/libs/xgboost/src/objective/regression_obj.cu:170: gpu:binary:logistic is now deprecated, use binary:logistic instead.\n",
      "4 fold's AUC: 0.9509904048356468\n",
      "4 fold's ACC: 0.881\n",
      "[17:48:15] WARNING: C:/dev/libs/xgboost/src/objective/regression_obj.cu:170: gpu:binary:logistic is now deprecated, use binary:logistic instead.\n",
      "5 fold's AUC: 0.9471876987571551\n",
      "5 fold's ACC: 0.8808\n",
      "====================================================================================================\n",
      "AUC_list\n",
      "[0.9519849217123374, 0.9487462390371935, 0.9466345726151382, 0.9509904048356468, 0.9471876987571551]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mean AUC: 0.9491087673914942\n",
      "====================================================================================================\n",
      "ACC_list\n",
      "[0.8848, 0.8776, 0.8764, 0.881, 0.8808]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mean ACC: 0.88012\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=config['k_folds'], random_state=config['seed'], shuffle=True)\n",
    "xgb_prediction1, xgb_auc_list1, xgb_acc_list1, xgb_S_prediction1 = xgb_kfold(x_data1, y_data, x_test1, folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:50:01] WARNING: C:/dev/libs/xgboost/src/objective/regression_obj.cu:170: gpu:binary:logistic is now deprecated, use binary:logistic instead.\n",
      "1 fold's AUC: 0.9422203255420101\n",
      "1 fold's ACC: 0.874\n",
      "[17:50:05] WARNING: C:/dev/libs/xgboost/src/objective/regression_obj.cu:170: gpu:binary:logistic is now deprecated, use binary:logistic instead.\n",
      "2 fold's AUC: 0.9361186863837142\n",
      "2 fold's ACC: 0.8578\n",
      "[17:50:09] WARNING: C:/dev/libs/xgboost/src/objective/regression_obj.cu:170: gpu:binary:logistic is now deprecated, use binary:logistic instead.\n",
      "3 fold's AUC: 0.940035814606292\n",
      "3 fold's ACC: 0.8692\n",
      "[17:50:13] WARNING: C:/dev/libs/xgboost/src/objective/regression_obj.cu:170: gpu:binary:logistic is now deprecated, use binary:logistic instead.\n",
      "4 fold's AUC: 0.9424573342470518\n",
      "4 fold's ACC: 0.8692\n",
      "[17:50:16] WARNING: C:/dev/libs/xgboost/src/objective/regression_obj.cu:170: gpu:binary:logistic is now deprecated, use binary:logistic instead.\n",
      "5 fold's AUC: 0.9409689148809358\n",
      "5 fold's ACC: 0.8664\n",
      "====================================================================================================\n",
      "AUC_list\n",
      "[0.9422203255420101, 0.9361186863837142, 0.940035814606292, 0.9424573342470518, 0.9409689148809358]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mean AUC: 0.9403602151320009\n",
      "====================================================================================================\n",
      "ACC_list\n",
      "[0.874, 0.8578, 0.8692, 0.8692, 0.8664]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mean ACC: 0.86732\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=config['k_folds'], random_state=config['seed'], shuffle=True)\n",
    "xgb_prediction2, xgb_auc_list2, xgb_acc_list2, xgb_S_prediction2 = xgb_kfold(x_data2, y_data, x_test2, folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:50:21] WARNING: C:/dev/libs/xgboost/src/objective/regression_obj.cu:170: gpu:binary:logistic is now deprecated, use binary:logistic instead.\n",
      "1 fold's AUC: 0.9451600731960682\n",
      "1 fold's ACC: 0.8794\n",
      "[17:50:39] WARNING: C:/dev/libs/xgboost/src/objective/regression_obj.cu:170: gpu:binary:logistic is now deprecated, use binary:logistic instead.\n",
      "2 fold's AUC: 0.9410829972472953\n",
      "2 fold's ACC: 0.8644\n",
      "[17:50:53] WARNING: C:/dev/libs/xgboost/src/objective/regression_obj.cu:170: gpu:binary:logistic is now deprecated, use binary:logistic instead.\n",
      "3 fold's AUC: 0.9411604610842558\n",
      "3 fold's ACC: 0.8724\n",
      "[17:51:11] WARNING: C:/dev/libs/xgboost/src/objective/regression_obj.cu:170: gpu:binary:logistic is now deprecated, use binary:logistic instead.\n",
      "4 fold's AUC: 0.9467065093163608\n",
      "4 fold's ACC: 0.8758\n",
      "[17:51:29] WARNING: C:/dev/libs/xgboost/src/objective/regression_obj.cu:170: gpu:binary:logistic is now deprecated, use binary:logistic instead.\n",
      "5 fold's AUC: 0.9401607257861283\n",
      "5 fold's ACC: 0.8692\n",
      "====================================================================================================\n",
      "AUC_list\n",
      "[0.9451600731960682, 0.9410829972472953, 0.9411604610842558, 0.9467065093163608, 0.9401607257861283]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mean AUC: 0.9428541533260217\n",
      "====================================================================================================\n",
      "ACC_list\n",
      "[0.8794, 0.8644, 0.8724, 0.8758, 0.8692]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mean ACC: 0.8722399999999999\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=config['k_folds'], random_state=config['seed'], shuffle=True)\n",
    "xgb_prediction3, xgb_auc_list3, xgb_acc_list3, xgb_S_prediction3 = xgb_kfold(x_data3, y_data, x_test3, folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 10000,\n",
    "    'max_depth': 3,\n",
    "    'num_leaves': 5,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 1.0,\n",
    "    'feature_fraction': 1.0,\n",
    "    'bagging_fraction': 1.0,\n",
    "    'reg_alpha': 0,\n",
    "    'reg_lambda': 1,\n",
    "    'min_child_weight': 3,\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_kfold(x_data, y_data, x_test, folds):\n",
    "    lgb_auc_list = list()\n",
    "    lgb_acc_list = list()\n",
    "    lgb_S_prediction = np.zeros(len(x_data))\n",
    "    lgb_prediction = np.zeros(len(x_test))\n",
    "\n",
    "    for fold, (train_idx, valid_idx) in enumerate(folds.split(X=x_data, y=y_data)):\n",
    "        x_train, y_train = x_data[train_idx,:], y_data[train_idx]\n",
    "        x_valid, y_valid = x_data[valid_idx,:], y_data[valid_idx] \n",
    "        lgb_model = lgb.LGBMClassifier(**lgb_params)\n",
    "        lgb_model.fit(x_train, y_train,\n",
    "                      eval_set=[(x_train, y_train), (x_valid, y_valid)],\n",
    "                      early_stopping_rounds=config['early_stopping_rounds'],\n",
    "                      verbose=False)\n",
    "\n",
    "        prob = lgb_model.predict_proba(x_valid, num_iteration=lgb_model.best_iteration_)[:,1]\n",
    "        test_prob = lgb_model.predict_proba(x_test, num_iteration=lgb_model.best_iteration_)[:,1]\n",
    "\n",
    "        auc = metrics.roc_auc_score(y_true=y_valid, y_score=prob)\n",
    "        pred = [1 if p > 0.5 else 0 for p in prob]\n",
    "        acc = metrics.accuracy_score(y_true=y_valid, y_pred=pred)\n",
    "        lgb_auc_list.append(auc)\n",
    "        lgb_acc_list.append(acc)\n",
    "        lgb_S_prediction[valid_idx] = prob\n",
    "        lgb_prediction += test_prob / folds.n_splits\n",
    "\n",
    "        print(\"{} fold's AUC: {}\".format(fold+1, auc))\n",
    "        print(\"{} fold's ACC: {}\".format(fold+1, acc))\n",
    "\n",
    "    print('='*100)\n",
    "    print('AUC_list')\n",
    "    print(lgb_auc_list)\n",
    "\n",
    "    print('-'*100)\n",
    "    print('Mean AUC: {}'.format(np.mean(lgb_auc_list)))\n",
    "\n",
    "    print('='*100)\n",
    "    print('ACC_list')\n",
    "    print(lgb_acc_list)\n",
    "\n",
    "    print('-'*100)\n",
    "    print('Mean ACC: {}'.format(np.mean(lgb_acc_list)))\n",
    "    \n",
    "    return lgb_prediction, lgb_auc_list, lgb_acc_list, lgb_S_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 fold's AUC: 0.9472599158078803\n",
      "1 fold's ACC: 0.87\n",
      "2 fold's AUC: 0.946468856027143\n",
      "2 fold's ACC: 0.8724\n",
      "3 fold's AUC: 0.9449108826866841\n",
      "3 fold's ACC: 0.874\n",
      "4 fold's AUC: 0.9479569255217546\n",
      "4 fold's ACC: 0.8744\n",
      "5 fold's AUC: 0.9450604221751984\n",
      "5 fold's ACC: 0.8748\n",
      "====================================================================================================\n",
      "AUC_list\n",
      "[0.9472599158078803, 0.946468856027143, 0.9449108826866841, 0.9479569255217546, 0.9450604221751984]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mean AUC: 0.9463314004437322\n",
      "====================================================================================================\n",
      "ACC_list\n",
      "[0.87, 0.8724, 0.874, 0.8744, 0.8748]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mean ACC: 0.8731200000000001\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=config['k_folds'], random_state=config['seed'], shuffle=True)\n",
    "lgb_prediction1, lgb_auc_list1, lgb_acc_list1, lgb_S_prediction1 = lgb_kfold(x_data1, y_data, x_test1, folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 fold's AUC: 0.9378817495626862\n",
      "1 fold's ACC: 0.8714\n",
      "2 fold's AUC: 0.9305446194225722\n",
      "2 fold's ACC: 0.8528\n",
      "3 fold's AUC: 0.9369804370073171\n",
      "3 fold's ACC: 0.8606\n",
      "4 fold's AUC: 0.9386568049921927\n",
      "4 fold's ACC: 0.867\n",
      "5 fold's AUC: 0.9353152713497684\n",
      "5 fold's ACC: 0.8566\n",
      "====================================================================================================\n",
      "AUC_list\n",
      "[0.9378817495626862, 0.9305446194225722, 0.9369804370073171, 0.9386568049921927, 0.9353152713497684]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mean AUC: 0.9358757764669073\n",
      "====================================================================================================\n",
      "ACC_list\n",
      "[0.8714, 0.8528, 0.8606, 0.867, 0.8566]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mean ACC: 0.86168\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=config['k_folds'], random_state=config['seed'], shuffle=True)\n",
    "lgb_prediction2, lgb_auc_list2, lgb_acc_list2, lgb_S_prediction2 = lgb_kfold(x_data2, y_data, x_test2, folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 fold's AUC: 0.9419857473762769\n",
      "1 fold's ACC: 0.8666\n",
      "2 fold's AUC: 0.9400518532744383\n",
      "2 fold's ACC: 0.8618\n",
      "3 fold's AUC: 0.9361735523596616\n",
      "3 fold's ACC: 0.866\n",
      "4 fold's AUC: 0.9386388847599465\n",
      "4 fold's ACC: 0.8634\n",
      "5 fold's AUC: 0.9374663887899964\n",
      "5 fold's ACC: 0.863\n",
      "====================================================================================================\n",
      "AUC_list\n",
      "[0.9419857473762769, 0.9400518532744383, 0.9361735523596616, 0.9386388847599465, 0.9374663887899964]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mean AUC: 0.938863285312064\n",
      "====================================================================================================\n",
      "ACC_list\n",
      "[0.8666, 0.8618, 0.866, 0.8634, 0.863]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mean ACC: 0.86416\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=config['k_folds'], random_state=config['seed'], shuffle=True)\n",
    "lgb_prediction3, lgb_auc_list3, lgb_acc_list3, lgb_S_prediction3 = lgb_kfold(x_data3, y_data, x_test3, folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_df(names, models, score_lst, score_name):\n",
    "    df = pd.DataFrame({'Name':names, 'Model':models, score_name:np.zeros(len(names))})\n",
    "    for i in range(len(names)):\n",
    "        mean_ = np.mean(score_lst[i])\n",
    "        std_ = np.std(score_lst[i])\n",
    "        df.loc[i,score_name] = '{0:.4f}({1:.4f})'.format(mean_, std_)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Model</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Part1</td>\n",
       "      <td>XGboost</td>\n",
       "      <td>0.9491(0.0021)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Part2</td>\n",
       "      <td>XGboost</td>\n",
       "      <td>0.9404(0.0023)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Part3</td>\n",
       "      <td>XGboost</td>\n",
       "      <td>0.9429(0.0026)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Part1</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.9463(0.0012)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Part2</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.9359(0.0029)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Part3</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.9389(0.0020)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Name     Model             AUC\n",
       "0  Part1   XGboost  0.9491(0.0021)\n",
       "1  Part2   XGboost  0.9404(0.0023)\n",
       "2  Part3   XGboost  0.9429(0.0026)\n",
       "3  Part1  LightGBM  0.9463(0.0012)\n",
       "4  Part2  LightGBM  0.9359(0.0029)\n",
       "5  Part3  LightGBM  0.9389(0.0020)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = ['Part1','Part2','Part3'] * 2\n",
    "models = ['XGboost'] * 3 + ['LightGBM'] * 3\n",
    "auc_score_lst = [xgb_auc_list1, xgb_auc_list2, xgb_auc_list3, lgb_auc_list1, lgb_auc_list2, lgb_auc_list3]\n",
    "auc_df = score_df(name, models, score_lst, score_name='AUC')\n",
    "auc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Model</th>\n",
       "      <th>ACC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Part1</td>\n",
       "      <td>XGboost</td>\n",
       "      <td>0.8801(0.0029)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Part2</td>\n",
       "      <td>XGboost</td>\n",
       "      <td>0.8673(0.0054)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Part3</td>\n",
       "      <td>XGboost</td>\n",
       "      <td>0.8722(0.0052)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Part1</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.8731(0.0018)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Part2</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.8617(0.0068)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Part3</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.8642(0.0018)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Name     Model             ACC\n",
       "0  Part1   XGboost  0.8801(0.0029)\n",
       "1  Part2   XGboost  0.8673(0.0054)\n",
       "2  Part3   XGboost  0.8722(0.0052)\n",
       "3  Part1  LightGBM  0.8731(0.0018)\n",
       "4  Part2  LightGBM  0.8617(0.0068)\n",
       "5  Part3  LightGBM  0.8642(0.0018)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = ['Part1','Part2','Part3'] * 2\n",
    "models = ['XGboost'] * 3 + ['LightGBM'] * 3\n",
    "acc_score_lst = [xgb_acc_list1, xgb_acc_list2, xgb_acc_list3, lgb_acc_list1, lgb_acc_list2, lgb_acc_list3]\n",
    "acc_df = score_df(name, models, score_lst, score_name='ACC')\n",
    "acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [xgb_prediction1, xgb_prediction2, xgb_prediction3, lgb_prediction1, lgb_prediction2, lgb_prediction3]\n",
    "for i in range(len(preds)):\n",
    "    output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":preds[i]})\n",
    "    output.to_csv(\"../submit/{0:s}+{1:s}+{2:.4f}.csv\".format(name[i], models[i], np.mean(score_lst[i])), index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'acc_score_lst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-513c99f624ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sentiment\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../submit/{0:s}+{1:s}+threshold_0.5_{2:.4f}.csv\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc_score_lst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'acc_score_lst' is not defined"
     ]
    }
   ],
   "source": [
    "preds = [xgb_prediction1, xgb_prediction2, xgb_prediction3, lgb_prediction1, lgb_prediction2, lgb_prediction3]\n",
    "for i in range(len(preds)):\n",
    "    output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":[1 if x > 0.5 else 0 for x in preds[i]]})\n",
    "    output.to_csv(\"../submit/{0:s}+{1:s}+threshold_0.5_{2:.4f}.csv\".format(name[i], models[i], np.mean(acc_score_lst[i])), index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.31078544, 0.02359064, 0.22037972, ..., 0.23561364, 0.20817214,\n",
       "       0.17305508])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_prediction1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
