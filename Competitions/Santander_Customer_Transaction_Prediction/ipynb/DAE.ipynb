{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wogur\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import keras.layers as KL\n",
    "import keras.models as KM\n",
    "import keras.callbacks as cb\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape:  (200000, 201)\n",
      "test.shape:  (200000, 200)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../dataset/train.csv', index_col=0)\n",
    "test = pd.read_csv('../dataset/test.csv', index_col=0)\n",
    "print('data.shape: ',data.shape)\n",
    "print('test.shape: ',test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = data.drop('target', axis=1)\n",
    "y_data = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAE:\n",
    "    def __init__(self, col_size):\n",
    "        self.col_size = col_size\n",
    "        self.inp = KL.Input((self.col_size,))\n",
    "        en = self.encoder()\n",
    "        de = self.decoder(en)\n",
    "        \n",
    "        self.model = KM.Model(self.inp, de)\n",
    "        \n",
    "    \n",
    "    def encoder(self):\n",
    "        x = KL.Dense(250)(self.inp)\n",
    "        x = KL.Dropout(0.4)(x)\n",
    "        x = KL.BatchNormalization()(x)\n",
    "        x = KL.Activation('relu')(x)\n",
    "        x = KL.Dense(250)(x)\n",
    "        x = KL.BatchNormalization()(x)\n",
    "        x = KL.Activation('tanh')(x)\n",
    "        x = KL.Dense(250)(x)\n",
    "        x = KL.BatchNormalization()(x)\n",
    "        en = KL.Activation('linear')(x)\n",
    "\n",
    "        return en\n",
    "\n",
    "    def decoder(self, en_output):\n",
    "        x = KL.Dense(250)(en_output)\n",
    "        x = KL.BatchNormalization()(x)\n",
    "        x = KL.Activation('relu')(x)\n",
    "        x = KL.Dense(250)(x)\n",
    "        x = KL.BatchNormalization()(x)\n",
    "        x = KL.Activation('tanh')(x)\n",
    "        x = KL.Dense(250)(x)\n",
    "        x = KL.BatchNormalization()(x)\n",
    "        x = KL.Activation('relu')(x)\n",
    "        x = KL.Dense(self.col_size)(x)\n",
    "        x = KL.BatchNormalization()(x)\n",
    "        de = KL.Activation('linear')(x)\n",
    "\n",
    "        return de\n",
    "\n",
    "    def feature_output(self, model):\n",
    "        en = self.encoder()\n",
    "        encoder = KM.Model(self.inp, en)\n",
    "        encoder.set_weights(model.get_weights())\n",
    "        return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler()\n",
    "# scaler.fit(x_data)\n",
    "# x_data_s = scaler.transform(x_data)\n",
    "# test_s = scaler.transform(test)\n",
    "# x_data_s = pd.DataFrame(x_data_s, columns=x_data.columns)\n",
    "# test_s = pd.DataFrame(test_s, columns=test.columns)\n",
    "# data_s = pd.concat([x_data_s, test_s], axis=0)\n",
    "# data_s.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([x_data, test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               50250     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 250)               1000      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 250)               1000      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 250)               1000      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 250)               1000      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 250)               1000      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 250)               1000      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 200)               50200     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 200)               0         \n",
      "=================================================================\n",
      "Total params: 421,000\n",
      "Trainable params: 417,600\n",
      "Non-trainable params: 3,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dae = DAE(data.shape[1])\n",
    "dae.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dae.model.compile(optimizer='adam',loss='mse')\n",
    "# dae.model.fit(data, data,\n",
    "#              batch_size=1024,\n",
    "#              epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dae.model.save('../dae_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dae_model = KM.load_model('../dae_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction.shape:  (400000, 200)\n",
      "mse:  13.547362402899564\n"
     ]
    }
   ],
   "source": [
    "# prediction = dae_model.predict(data)\n",
    "# print('prediction.shape: ',prediction.shape)\n",
    "# print('mse: ',np.mean(np.mean(np.square(prediction-data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mse:  13.543876897062244\n",
      "test mse:  13.55084790873696\n"
     ]
    }
   ],
   "source": [
    "# new_train = dae_model.predict(x_data)\n",
    "# new_test = dae_model.predict(test)\n",
    "# print('train mse: ',np.mean(np.mean(np.square(new_train-x_data))))\n",
    "# print('test mse: ',np.mean(np.mean(np.square(new_test-test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 250)               50250     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 250)               1000      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 250)               1000      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 250)               1000      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 250)               0         \n",
      "=================================================================\n",
      "Total params: 178,750\n",
      "Trainable params: 177,250\n",
      "Non-trainable params: 1,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder = dae.feature_output(dae_model)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = encoder.predict(x_data)\n",
    "new_test = encoder.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = ['val_{}'.format(i) for i in range(new_train.shape[1])]\n",
    "new_train = pd.DataFrame(new_train, columns=colnames)\n",
    "new_test = pd.DataFrame(new_test, columns=colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_train.shape:  (200000, 250)\n",
      "new_test.shape:  (200000, 250)\n"
     ]
    }
   ],
   "source": [
    "print('new_train.shape: ',new_train.shape)\n",
    "print('new_test.shape: ',new_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"seed\":2019,\n",
    "    \"k_folds\":5,\n",
    "    \"early_stopping_rounds\":100\n",
    "}\n",
    "\n",
    "params = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"n_estimators\": 10000,\n",
    "    \"max_depth\": 3,\n",
    "    \"min_child_weight\": 5,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"colsample_bylevel\": 0.8,\n",
    "    \"alpha\": 0,\n",
    "    \"lambda\": 10,\n",
    "    \"objective\": \"gpu:binary:logistic\",\n",
    "    \"tree_method\": \"gpu_hist\",\n",
    "    \"predictor\": \"gpu_predictor\",\n",
    "    \"eval_metric\":\"auc\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = StratifiedKFold(n_splits=config['k_folds'], random_state=config['seed'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "0 fold\n"
     ]
    }
   ],
   "source": [
    "auc_list = list()\n",
    "probs = np.zeros(len(test))\n",
    "\n",
    "fold_score_df = pd.DataFrame\n",
    "for i, (train_idx, valid_idx) in enumerate(folds.split(X=new_train, y=y_data)):\n",
    "    print('='*25)\n",
    "    print('{} fold'.format(i))\n",
    "    x_train, y_train = new_train.iloc[train_idx, :], y_data.iloc[train_idx]\n",
    "    x_valid, y_valid = new_train.iloc[valid_idx, :], y_data.iloc[valid_idx]\n",
    "\n",
    "    watchlist = [(x_train,y_train),(x_valid, y_valid)]\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(x_train, y_train,\n",
    "            eval_set=watchlist,\n",
    "            early_stopping_rounds=config['early_stopping_rounds'],\n",
    "            verbose=100)\n",
    "\n",
    "    val_prob = model.predict_proba(x_valid, ntree_limit=model.best_iteration)[:,1]\n",
    "    prob = model.predict_proba(new_test, ntree_limit=model.best_iteration)[:,1]\n",
    "    \n",
    "    val_auc = metrics.roc_auc_score(y_valid, val_prob)\n",
    "    auc_list.append(val_auc)\n",
    "    print('val AUC: ',val_auc)\n",
    "\n",
    "    probs = prob/folds.get_n_splits()\n",
    "    \n",
    "    score_df = pd.DataFrame()\n",
    "    feature_score =  model.get_booster().get_score(importance_type='gain')\n",
    "    score_df.loc[:,'feature'] = list(feature_score.keys())\n",
    "    score_df.loc[:,'importance'] = list(feature_score.values())\n",
    "    fold_score_df = pd.concat([fold_score_df, score_df])\n",
    "    \n",
    "\n",
    "print('='*100)\n",
    "print('AUC_LIST')\n",
    "print(auc_list)\n",
    "\n",
    "print('-'*100)\n",
    "print('Mean AUC: {}'.format(np.mean(auc_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
